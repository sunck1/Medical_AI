# Medical_AI
README

Codes link: https://github.com/sunck1/Medical_AI

Title of the project

1.The code was used to perform on digits recognition and borrower prediction. I deployed 5 types of algorithms, Decision Tree, SVM, XGBoost,
KNN, and Neural Network. All the models came from the SK-learn package. I also adopted
the Bayes optimization to tune the hyperparameters, with the iteration of 10, on the
algorithms except for the Neural Network. The accuracy was used for the evaluation metric
when obtaining the optimal hyperparameters. The accuracy is the main evaluation metric. If there is same accurate, the test time consumption is used for evaluation. 




2. For this assignment, I utilized the two datasets from the book, Machine Learning in
Action (link:https://github.com/TeFuirnever/Machine-Learning-in-Action/) The first task is digit recognition. 10 digits (0-9) were generated by 2-bit
characters (0,1). The size of a digit was 32×32. All the digits were stored with txt. files,
covering 1934 training samples and 946 test samples. The file name is the label and the picture of the 32×32 pixels is what the number looks like(e.g, label). Each pixel is represented by characters of 0 and 1. Then, the target of this task is to recognize which digit the file is. The sample of the digits is shown below. The second task is a simple classification, with 15 samples
of borrowers(embedding in the codes). Four features were recorded, including ages, have_job, have_house, and
credits. For ages, 0 represents the young, 1 the middle, and 2 the old. For the have_job
category, 1 denotes the person who has a job and 0 for not. For the have_house, 1 means
the person who has a house and 0 for not. For the credits part, 2 represents high credits, 0
for the lowest. For the labels, 1 means we can offer them money, 0 for not.
The first task is multi-classification, and the second is binary classification. Plus, the
digits prediction contains high-dimensional features(each pixel denotes one feature) and the key predictors of this task are less obvious. Thus, the quality of machine learning models trained on both small and large, complex and simple datasets was investigated.

3 Instructions on how to run your code 
3.1 Including any prerequisites such as installing Python 3.9.4
3.2 Environment is Jupiter notebook on MAC. Required packages are in the requirements.txt
3.3 The Homework1 is for task1 digits recognition, homework1_2 for task2, borrowers prediction. Both notebooks contain (SVM, XGBoost, DT, MLP, KNN)
3.4 
Task 1. Digits recognition
Based on the performance, the KNN model got the best performance with 0.987
accuracy, recall, precision, F1 score. Meanwhile, both the training consumption and
prediction consumption are less than the second-rank model – SVM. The training
consumption and prediction consumption are 0.0098 and 0.1463 on the KNN and 1.37 and
0.23 on the SVM. Thus, I picked the KNN as the best model.
Task2. Borrowers classification
According to the performance, all the models get perfect results — 1 accuracy,
recall, precision, F1 score. However, the decision tree cost the least training and prediction
time. Thus, I picked the decision tree as the best model.
In conclusion, for the multi-classification tasks, KNN performed great. Because the
KNN would group all the similar cases instead of differentiating them and finding the
hyperplane. For simple tasks, the tree model performed the best on the task with
low-dimensional features. Usually, tree models cost less than linear models. And, more data
would dig out the potential of machine learning algorithms, based on the error rate analysis.


